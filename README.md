# Spring AI and Ollama: Getting Started

This guide walks you through setting up a simple Spring Boot application that uses the local Ollama service and a large language model (LLM) to respond to chat queries.

## Prerequisites

Before you begin, make sure you have the following installed:

  * **Java 21**
  * **Maven**
  * **Ollama**: [https://ollama.com/]

## Step 1: Set up Ollama Locally

1.  **Install Ollama**: Download and install Ollama from the official web-site. It will start a local server that runs LLMs.

2.  **Pull an LLM**: Open your terminal or command prompt and pull a model to use locally. I have used `llama3.2` model.

```
    ollama pull llama3.2
```

   Ollama will download the model's weights to your machine. This may take some time depending on your connection.

## Step 2: Create the Spring Boot Project

1.  **Use Spring Initializr**: Go to [https://start.spring.io/](https://start.spring.io/) to generate a new project.

2.  **Project Configuration**:

      * **Project**: Maven Project
      * **Language**: Java
      * **Spring Boot**: 3.5.5 or a compatible version
      * **Java**: 21
      * **Dependencies**: Add **Spring Web** and **Spring AI Ollama Starter**.

3.  Click **Generate** and unzip the downloaded project file. Open the project in your favorite IDE.

## Step 3: Configure the Application

1.  **Check **` pom.xml` : Verify that your `pom.xml` file includes the necessary dependencies. You should see `spring-boot-starter-web` and `spring-ai-ollama-spring-boot-starter`.

        
```xml
    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <dependency>
			<groupId>org.springframework.ai</groupId>
			<artifactId>spring-ai-starter-model-ollama</artifactId>
		 </dependency>
    </dependencies>
```

2.  **Configure** `application.properties`: In `src/main/resources/application.properties`, add the configuration to point to your local Ollama server and specify the model to use. 
	The default URL is `http://localhost:11434`.
     
	    
    spring.ai.ollama.base-url=http://localhost:11434
    spring.ai.ollama.chat.model=llama3.2:3b
    

## Step 4: Write the Java Code

1.  **Create a Chat Controller**: Create a new class to handle incoming HTTP requests and interact with the LLM. The `ChatClient` is the core component that abstracts away the specific LLM provider.

 		
```java
	import org.springframework.ai.chat.client.ChatClient;
	import org.springframework.web.bind.annotation.GetMapping;
	import org.springframework.web.bind.annotation.RequestParam;
	import org.springframework.web.bind.annotation.RestController;
	
	import lombok.RequiredArgsConstructor;
	
	@RestController
	@RequiredArgsConstructor
	public class OllamaChatController {
		
		private final ChatClient ollamaChatClient;
	
		
		@GetMapping("/ollama/chat")
		public String generateResponse(@RequestParam String message) {
			return ollamaChatClient.prompt().user(message).call().content();
		}
	}
	
	import org.springframework.ai.chat.client.ChatClient;
	import org.springframework.ai.ollama.OllamaChatModel;
	import org.springframework.boot.CommandLineRunner;
	import org.springframework.context.annotation.Bean;
	import org.springframework.context.annotation.Configuration;
	@Configuration
	public class OllamaChatConfig {

		@Bean
		ChatClient ollamaChatClient(OllamaChatModel chatModel) {
			return ChatClient.builder(chatModel).build();
		}


	}

```
2.  **Create OllamaChatConfig**: Gives a ChatClient Bean.

```java
	
	import org.springframework.ai.chat.client.ChatClient;
	import org.springframework.ai.ollama.OllamaChatModel;
	import org.springframework.boot.CommandLineRunner;
	import org.springframework.context.annotation.Bean;
	import org.springframework.context.annotation.Configuration;
	@Configuration
	public class OllamaChatConfig {

		@Bean
		ChatClient ollamaChatClient(OllamaChatModel chatModel) {
			return ChatClient.builder(chatModel).build();
		}


	}

```
## Step 5: Run the Application

1.  **Start the Application**: Run your Spring Boot application from your IDE. Make sure the Ollama server is running in the background.

2.  **Test the Endpoint**: Open your web browser or a tool like `curl` and send a request to your endpoint.

```bash
    curl "http://localhost:8080/ollama/chat?message=What is Spring AI?"
```

   You should receive a response generated by your local `llama3` model.